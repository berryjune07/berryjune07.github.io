<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-08-18T04:43:20+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jiho’s Blog</title><subtitle>noting down my thoughts and experiences
</subtitle><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><entry><title type="html">Work and Energy &amp;amp; Conservations</title><link href="http://localhost:4000/physics/work-and-energy-and-conservations.html" rel="alternate" type="text/html" title="Work and Energy &amp;amp; Conservations" /><published>2025-08-14T00:00:00+09:00</published><updated>2025-08-14T00:00:00+09:00</updated><id>http://localhost:4000/physics/work-and-energy-and-conservations</id><content type="html" xml:base="http://localhost:4000/physics/work-and-energy-and-conservations.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Work

**Work** is defined as the product of the force applied to an object and the distance over which that force is applied, in the direction of the force.
The formula for work is given by:

\\[
W_{12} = \int_{\b{r}_1}^{\b{r}_2} \b{F} \cdot \dd{\b{s}}
\\]

It is often denoted as $W$, and its SI unit is the **joule** ($\mathrm{J=N \cdot m= kg \cdot m^2 / s^2}$).

### Power

**Power** is the rate at which work is done or energy is transferred.
It is defined as:

\\[
P = \odv{W}{t}
\\]

Alternatively, power can also be expressed in terms of force and velocity:

\\[
\begin{align\*}
P &amp;= \odv{W}{t} \nl
&amp;= \odv{}{t} \int \b{F} \cdot \odv{\b{s}}{t} \dd{t} \nl
&amp;= \b{F} \cdot \b{v}
\end{align\*}
\\]

It is often denoted as $P$, and its SI unit is the **watt** ($\mathrm{W=J/s= kg \cdot m^2 / s^3}$).

## Kinetic Energy

By the Newton&apos;s second law,

\\[
\begin{align\*}
W_{12} &amp;= \int\_{\b{r}\_1}^{\b{r}\_2} \b{F} \cdot \dd{\b{s}} \nl
&amp;= \int\_{\b{r}\_1}^{\b{r}\_2} m \dot{\b{v}} \cdot \odv{\b{s}}{t} \dd{t} \nl
&amp;= \int\_{t_1}^{t_2} m \dot{\b{v}} \cdot \dd{\b{v}} \nl
&amp;= \int\_{\b{v}\_1}^{\b{v}\_2} m \dd{ \left( \frac{1}{2} \abs{\b{v}}^2 \right) } \nl
&amp;= \frac{1}{2} m (v_2^2 - v_1^2)
\end{align\*}
\\]

The **kinetic energy** of an object is defined as:

\\[
K = \frac{1}{2} m v^2
\\]

Then we get:

\\[
W = \Delta K
\\]

This is known as the **work-energy theorem**.
We can conclude that energy is something that can be converted into work,
and work is the transfer of energy.

## Conservative Forces

A force is said to be **conservative** if the work done by the force on an object moving from one point to another is independent of the path taken.
For conservative forces, the work done is only dependent on the initial and final positions of the object.
In other words,

\\[
\oint \b{F}_C \cdot \dd{\b{s}} = 0
\\]

By the mathematical analysis of the property, a conservative force can be expressed as the negative gradient of some scalar function $U$:

\\[
\b{F}_C = -\grad U
\\]

and $U$ is called the **potential energy** by the conservative force.

### Potential Energy

The **potential energy** associated with a conservative force is defined as the work done by the force when moving an object from a reference point to a specific point in space.
The potential energy is given by:

\\[
U(\b{r}) = -\int_{\b{r}_0}^{\b{r}} \b{F}_C \cdot \dd{\b{s}}
\\]

where $\b{r}_0$ is the reference point and can be chosen arbitrarily.
The work done by the conservative force is then:

\\[
W = -\Delta U
\\]

We have some common examples of conservative forces and their associated potential energies:
- The gravitational force $\;\b{F} = -m g \hat{\b{z}} \;$ has the potential energy $\;U = m g z$.
- The elastic force of a spring $\;\b{F} = -k \b{x} \;$ has the potential energy $\;U = \frac{1}{2} k x^2$.

### Mechanical Energy

The **mechanical energy** of a system is the sum of its kinetic energy and potential energy:

\\[
E = K + U
\\]

## Conservation Theorems

### Linear Momentum Conservation

The **linear momentum** of an object is conserved if the net external force acting on the object is zero.

\\[
\dot{\b{p}} = \b{F}_{\text{net}}
\\]

This theorem can be analogized to the conservation of the linear momentum of a system of particles,
which is trivial by the Newton&apos;s third law.

### Mechanical Energy Conservation

The **mechanical energy** of a system is conserved if the only forces acting on the system are conservative forces.
In this case, the total mechanical energy remains constant:

\\[
W = \Delta K = -\Delta U \implies \Delta (K + U) = \Delta E = 0
\\]</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="physics" /><category term="elementary-physics" /><summary type="html"></summary></entry><entry><title type="html">Constraints of Coordinates</title><link href="http://localhost:4000/physics/constraints-of-coordinates.html" rel="alternate" type="text/html" title="Constraints of Coordinates" /><published>2025-08-13T00:00:00+09:00</published><updated>2025-08-13T00:00:00+09:00</updated><id>http://localhost:4000/physics/constraints-of-coordinates</id><content type="html" xml:base="http://localhost:4000/physics/constraints-of-coordinates.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Constraints

A **constraint** is a condition that restricts the motion of a system.
For example, a particle constrained to move along a straight line, or a gas confined to a cylinder.
Constraints can be classified to two main types.

### Holonomic Constraints

A **holonomic constraint** is a constraint that can be expressed as an equation involving the coordinates of the system:

\\[
f(\b{r}_1, \b{r}_2, \ldots, \b{r}_n, t) = 0
\\]

where $\b{r}_i$ are the coordinates of the particles in the system.
The simplest example is a rigid body, where the distance between any two points is constant.
Holonomic constraints can be classified further into two types again.

#### Rheonomic Constraints

A **rheonomic constraint** is a holonomic constraint that the equation of the constraint explicitly depends on time.
Mechanical systems with such constraints are called **rheonomous**.

#### Scleronomic Constraints

A **scleronomic constraint** is a holonomic constraint that the equation of the constraint does not explicitly depend on time.
Mechanical systems with such constraints are called **scleronomous**.

### Non-Holonomic Constraints

A **non-holonomic constraint** is a constraint that cannot be expressed as an equation involving the coordinates of the system.
A gas confined to a cylinder is an example of a non-holonomic constraint, as the gas can move freely in the cylinder but cannot escape it,
so that the constraint is expressed in terms of inequalities rather than equations.

## Generalized Coordinates

A **generalized coordinate** system is a coordinate system that describes the configuration of a system in terms of its degrees of freedom.
A **degree of freedom** is a parameter that can be varied independently to describe the configuration of a system:
mechanical systems with $N$ particles and $M$ holonomic constraints have $3N - M$ degrees of freedom since
each particle has 3 degrees of freedom in 3D space, and each holonomic constraint reduces the degrees of freedom by 1.
By denoting the generalized coordinates as $q_1, q_2, \ldots, q_n$, where $n = 3N - M$,

\\[
\begin{align\*}
\b{r}_1 &amp;= \b{r}_1(q_1, q_2, \ldots, q_n, t) \nl
&amp; \;\; \vdots \nl
\b{r}_N &amp;= \b{r}_N(q_1, q_2, \ldots, q_n, t)
\end{align\*}
\\]

The generalized coordinates can be used to describe the configuration of the system in terms of its degrees of freedom.
It doesn&apos;t have to be the Cartesian coordinates, even doesn&apos;t have to be the linear or orthogonal coordinates,
or has the dimension of the space. 

Advantage of generalized coordinates comes from the independence of the coordinates.
Formal space coordinates are independent of each other only if the system is free of constraints.
In generalized coordinates, the coordinates are independent of each other even if the system is constrained.

### Pfaffian Constraints

A **Pfaffian constraint** is a non-holonomic constraint that can be expressed as a linear combination of the differentials of the generalized coordinates:

\\[
\sum_{i=1}^{n} A_{ri} \dd{q_i} + B_r \dd{t} = 0
\\]

where $A_{ri}$ and $B_r$ are functions of the generalized coordinates and time.
Suppose that we&apos;re arguing with a holonomic system with the following constraints:

\\[
f_r(q_1, q_2, \ldots, q_n, t) = 0
\\]

Then we can differentiate the equation with respect to time:

\\[
\sum_{i=1}^{n} \pdv{f_r}{q_i} \dd{q_i} + \pdv{f_r}{t} \dd{t} = 0
\\]

And we see that the form of the equation is the same as the Pfaffian constraint form.
This means that holonomic constraints can always be expressed in Pfaffian form, but not all Pfaffian constraints are holonomic.
A Pfaffian constraint is holonomic if and only if the equation can be _integrated_ to yield a holonomic constraint.</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="physics" /><category term="classical-mechanics" /><summary type="html"></summary></entry><entry><title type="html">D’Alembert’s Principle and Lagrange’s Equations</title><link href="http://localhost:4000/physics/dalemberts-principle-and-lagranges-equations.html" rel="alternate" type="text/html" title="D’Alembert’s Principle and Lagrange’s Equations" /><published>2025-08-13T00:00:00+09:00</published><updated>2025-08-13T00:00:00+09:00</updated><id>http://localhost:4000/physics/dalemberts-principle-and-lagranges-equations</id><content type="html" xml:base="http://localhost:4000/physics/dalemberts-principle-and-lagranges-equations.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="physics" /><category term="classical-mechanics" /><summary type="html"></summary></entry><entry><title type="html">Mechanics of a System of Particles</title><link href="http://localhost:4000/physics/mechanics-of-a-system-of-particles.html" rel="alternate" type="text/html" title="Mechanics of a System of Particles" /><published>2025-08-12T00:00:00+09:00</published><updated>2025-08-12T00:00:00+09:00</updated><id>http://localhost:4000/physics/mechanics-of-a-system-of-particles</id><content type="html" xml:base="http://localhost:4000/physics/mechanics-of-a-system-of-particles.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Mechanics of a Particle

Let $\b{r}$ be the position vector of a particle, then the velocity $\b{v}$ and acceleration $\b{a}$ are given by

\\[
\begin{align\*}
\b{v} &amp;= \odv{\b{r}}{t} \nl
\b{a} &amp;= \odv{\b{v}}{t} = \odvn{2}{ \b{r} }{t}
\end{align\*}
\\]

The linear momentum $\b{p}$ of a particle with mass $m$ is defined as

\\[
\b{p} = m\b{v}
\\]

The mechanics of the particle is contained in _Newton&apos;s second law of motion_,
which states that there exists frames of reference in which the motion of a particle is described by

\\[
\b{F} = \odv{\b{p}}{t}
\\]

where $\b{F}$ is the net force acting on the particle.
Such a frame of reference is called an _inertial_ or _Galilean system_.
If the mass $m$ is constant, we can also write this as

\\[
\b{F} = m\odv{\b{v}}{t} = m\b{a}
\\]

Many of the important conclusions of mechanics can be expressed in the form of conservation theorems,
and the first is the _conservation of linear momentum of a particle_:
&gt; If $\b{F} = 0$, then $\b{p}$ is conserved.

The angular momentum $\b{L}$ of a particle with respect to a point $\b{O}$ is defined as

\\[
\b{L} = \b{r} \times \b{p}
\\]

where $\b{r}$ is the position vector of the particle with respect to the point $\b{O}$.
The moment of force or torque $\b{N}$ with respect to the point $\b{O}$ is defined as

\\[
\b{N} = \b{r} \times \b{F}
\\]

We can also express the angular momentum in terms of the torque:

\\[
\begin{align\*}
\odv{\b{L}}{t} &amp;= \odv{}{t} (\b{r} \times \b{p}) \nl
&amp;= \b{v} \times m\b{v} + \b{r} \times \b{F} \nl
&amp;= \b{N}
\end{align\*}
\\]

And we similarly have the _conservation of angular momentum of a particle_:
&gt; If $\b{N} = 0$, then $\b{L}$ is conserved.

Next, the work done by a force $\b{F}$ on a particle is defined as

\\[
W_{12} = \int_1^2 \b{F} \cdot \dd{\b{s}}
\\]

If the mass $m$ is constant, we can also write this as

\\[
\begin{align\*}
W_{12} &amp;= \int_1^2 m\odv{\b{v}}{t} \cdot \odv{\b{r}}{t} \dd{t} \nl
&amp;= m\int_1^2 \odv{}{t} \left( \frac{v^2}{2} \right) \dd{t} \nl
&amp;= \frac{m}{2} \left( v_2^2 - v_1^2 \right)
\end{align\*}
\\]

Thus we define the _kinetic energy_ $T$ of a particle as

\\[
T = \frac{1}{2}mv^2
\\]

and get the _work-energy theorem_:

\\[
W = \Delta T
\\]

Let&apos;s talk about the _conservative forces_.
A force $\b{F}$ is called conservative if the work done by the force from a fixed point to another fixed point is independent of the path taken.
This is equivalent with:

\\[
\oint \b{F} \cdot \dd{\b{s}} = 0
\\]

And by the Stokes theorem, this is equivalent with $\curl \b{F} = 0$.
Thus, by the Poincaré lemma, we can write the force as the gradient of a scalar function $U$:

\\[
\b{F} = -\grad U
\\]

The function $U(\b{r})$ is called the _potential energy_ of the force, and can be
calculated by setting the potential energy at a fixed point to zero:

\\[
U(\b{r}) = -\int_{\b{r}_0}^{\b{r}} \b{F} \cdot \dd{\b{s}}
\\]

By the gradient theorem, we can write the work done by a conservative force as

\\[
W = -\Delta U
\\]

Therefore, we can also write the work-energy theorem as

\\[
\Delta (T + U) = 0
\\]

This means that the sum of the kinetic energy and potential energy is conserved, and we call this the _total mechanical energy_.
However, if a force is conservative but depends explicitly on time, then the total mechanical energy is not conserved;
physically, displacement(path integral) is nonzero only when time flows.

## Mechanics of a System of Particles

In generalizing the mechanics of a particle to a system of particles,
we must distinguish between the _external forces_ acting on the system and the _internal forces_ acting between the particles of the system.
The equation of motion for the $i$-th particle is given by:

\\[
\dot{\b{p}}\_i = \b{F}\_i^\text{ext} + \sum_{j \neq i} \b{F}\_{ij}
\\]

where $\b{F}\_i^\text{ext}$ is the external force acting on the $i$-th particle,
and $\b{F}\_{ij}$ is the internal force acting on the $i$-th particle due to the $j$-th particle.
Summing over all particles, we get the equation of motion for the system:

\\[
\odvn{2}{}{t} \sum\_i m\_i \b{r}\_i = \sum\_i \b{F}\_i^\text{ext} + \sum\_{i\neq j} \b{F}\_{ij}
\\]

By the _Newton&apos;s third law of motion_, the internal forces satisfy $\b{F}\_{ij} = -\b{F}\_{ji}$, and thus the sum of the internal forces is zero.
Writing the total mass of the system as $M = \sum_i m_i$ and the _center of mass_ $\b{R}$ as

\\[
\b{R} = \frac{1}{M} \sum_i m_i \b{r}_i
\\]

we can rewrite the equation of motion for the system as

\\[
M \odvn{2}{}{t} \b{R} = \sum_i \b{F}_i^\text{ext} = \b{F}^\text{ext}
\\]

This means that the center of mass of the system behaves like a single particle with mass $M$ under the influence of the external forces.
The total linear momentum of the system is given by

\\[
\b{P} = \sum_i m_i \odv{\b{r}_i}{t} = M \b{V}
\\]

where $\b{V} = \odv{\b{R}}{t}$ is the velocity of the center of mass.
And we have the _conservation of linear momentum of a system of particles_:
&gt; If $\b{F}^\text{ext} = 0$, then $\b{P}$ is conserved.

The total angular momentum of the system with respect to a point $\b{O}$ is given by

\\[
\b{L} = \sum_i \b{r}_i \times \b{p}_i
\\]

Let&apos;s define $\b{r}_{ij} = \b{r}_i - \b{r}_j$ as the position vector of the $i$-th particle with respect to the $j$-th particle.

\\[
\begin{align\*}
\odv{\b{L}}{t} &amp;= \sum_i \odv{}{t} (\b{r}\_i \times m_i \odv{\b{r}\_i}{t}) \nl
&amp;= \sum_i \b{r}\_i \times \b{F}\_i^\text{ext} + \sum_{i\neq j} \b{r}\_i \times \b{F}\_{ij} \nl
&amp;= \sum_i \b{r}\_i \times \b{F}\_i^\text{ext} + \sum_{i &lt; j} \b{r}\_{ij} \times \b{F}\_{ij}
\end{align\*}
\\]

If the internal forces between the particles, in addition to being equal and opposite,
also lie along the line joining the particles -- a condition known as the _strong form of Newton&apos;s third law_ -- then the second term vanishes.
In this case, we can write the angular momentum of the system as

\\[
\odv{\b{L}}{t} = \sum_i \b{r}_i \times \b{F}_i^\text{ext} = \b{N}^\text{ext}
\\]

And we have the _conservation of angular momentum of a system of particles_:
&gt; If $\b{N}^\text{ext} = 0$, then $\b{L}$ is conserved.

Let&apos;s see the system from the point of view of the center of mass.
Let&apos;s define

\\[
\b{r}^\prime_i = \b{r}_i - \b{R} \nl
\b{v}^\prime_i = \b{v}_i - \b{V}
\\]

Then we have a good property:

\\[
\sum_i m_i \b{r}^\prime_i = 0 \nl
\sum_i m_i \b{v}^\prime_i = 0
\\]

This means that the center of mass is at the origin in this new coordinate system.
Using this, we can rewrite the angular momentum of the system as

\\[
\begin{align\*}
\b{L} &amp;= \sum_i \left( \b{r}^\prime_i + \b{R} \right) \times m_i \left( \b{v}^\prime_i + \b{V} \right) \nl
&amp;= \b{R} \times M\b{V} + \sum_i \b{r}^\prime_i \times m_i \b{v}^\prime_i
\end{align\*}
\\]

By writing $\b{p}^\prime_i = m_i \b{v}^\prime_i$,

\\[
\b{L} = \b{R} \times \b{P} + \sum_i \b{r}^\prime_i \times \b{p}^\prime_i
\\]

This means that the angular momentum of the system can be decomposed into two parts:
1. The angular momentum of the center of mass
2. The angular momentum of the particles with respect to the center of mass

This emphasizes that $\b{L}$ depends on the choice of the point $\b{O}$, only through $\b{R}$.
Only if $b{R}$ is rest with respect to $\b{O}$, $\b{L}$ will be independent of the point of reference.

The total kinetic energy of the system is given by

\\[
\begin{align\*}
T &amp;= \sum_i T_i = \sum_i \frac{1}{2} m_i \left\vert \b{v}_i^\prime + \b{V} \right\vert^2 \nl
&amp;= \sum_i \frac{1}{2} m_i \left( v_i^{\prime 2} + 2\b{v}_i^\prime \cdot \b{V} + V^2 \right) \nl
&amp;= \frac{1}{2} M V^2 + \sum_i \frac{1}{2} m_i v_i^{\prime 2}
\end{align\*}
\\]

This means that the total kinetic energy of the system can also be decomposed into two parts:
1. The kinetic energy of the center of mass
2. The kinetic energy of the particles with respect to the center of mass

Now let&apos;s seek for the work done by the forces acting on the system.
Assume that every force acting on the system is conservative, then

\\[
\begin{align\*}
W_{12} &amp;= \sum_i \int_1^2 \b{F}\_i^\text{ext} \cdot \dd{\b{s}}_i + \sum\_{i \neq j} \int_1^2 \b{F}\_{ij} \cdot \dd{\b{s}}\_i \nl
&amp;= -\sum_i \int_1^2 \grad_i U_i \cdot \dd{\b{s}}\_i - \sum\_{i \neq j} \int_1^2 \grad_i U\_{ij} \cdot \dd{\b{s}}\_i
\end{align\*}
\\]

where $U_i$ is the potential energy of the external force acting on the $i$-th particle,
and $U_{ij}$ is the potential energy of the internal force acting between the $i$-th and $j$-th particles.
$\grad_i$ means that we take the gradient with respect to the coordinates of the $i$-th particle.
By the Newton&apos;s third law, we have $\grad_i U_{ij} = -\grad_j U_{ji}$,

\\[
\begin{align\*}
W_{12} &amp;= -\sum_i \Delta U_i - \sum_{i &lt; j} \int_1^2 \left( \grad_i U_{ij} \cdot \dd{\b{s}}\_i + \grad_j U_{ji} \cdot \dd{\b{s}}\_j \right) \nl
&amp;= -\sum_i \Delta U_i - \sum\_{i &lt; j} \int_1^2 \grad_i U\_{ij} \cdot \left( \dd{\b{s}}\_i - \dd{\b{s}}\_j \right) \nl
&amp;= -\sum_i \Delta U_i - \sum\_{i &lt; j} \int_1^2 \grad_{ij} U\_{ij} \cdot \dd{\b{s}}\_{ij} \nl
&amp;= -\sum_i \Delta U_i - \sum\_{i &lt; j} \Delta U_{ij}
\end{align\*}
\\]

This means that we can write the whole potential energy of the system as

\\[
U = \sum_i U_i + \sum_{i &lt; j} U_{ij}
\\]

We often set $U_{ij}=U_{ji}$, so that the potential energy is symmetric with respect to the particles.

\\[
U = \sum_i U_i + \frac{1}{2} \sum_{i \neq j} U_{ij}
\\]

Thus, we can write the work done by the forces acting on the system as

\\[
W = -\Delta U
\\]

And also get the _conservation of total mechanical energy_:

\\[
\Delta (T + U) = 0
\\]

Let&apos;s look for the internal potential energy more closely.
If it depends only on the distance between the particles, i.e. $U_{ij} = U_{ij}(r_{ij})$,
then we can write the force as

\\[
\b{F}\_{ij} = -\grad\_i U\_{ij}(r\_{ij}) = -\pdv{U\_{ij}}{r\_{ij}} \grad\_i r\_{ij} = -\pdv{U\_{ij}}{r\_{ij}} \frac{\b{r}\_{ij}}{r\_{ij}}
\\]

This means that the internal forces are central forces, and they satisfy the strong form of Newton&apos;s third law.
Internal potential is generally not zero, and it may vary as the system changes with time.
However, for rigid bodies, in other words for constant $r\_{ij}$,
$\dd{\b{s}}\_{ij}$ can only be perpendicular to $\b{r}\_{ij}$, and so for central forces, it is perpendicular to $\b{F}\_{ij}$,
which means that the work done by the internal forces is zero, resulting in the constant internal potential energy.
Thus, we can completely disregard the internal potential energy of a rigid body system.
&gt; $U = \sum_i U_i$ for a rigid body system.</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="physics" /><category term="classical-mechanics" /><summary type="html"></summary></entry><entry><title type="html">Order Statistics</title><link href="http://localhost:4000/mathematics/order-statistics.html" rel="alternate" type="text/html" title="Order Statistics" /><published>2025-08-06T00:00:00+09:00</published><updated>2025-08-06T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/order-statistics</id><content type="html" xml:base="http://localhost:4000/mathematics/order-statistics.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

##</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="statistics" /><summary type="html"></summary></entry><entry><title type="html">Random Samples and the Statistics</title><link href="http://localhost:4000/mathematics/random-samples-and-the-statistics.html" rel="alternate" type="text/html" title="Random Samples and the Statistics" /><published>2025-08-05T00:00:00+09:00</published><updated>2025-08-05T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/random-samples-and-the-statistics</id><content type="html" xml:base="http://localhost:4000/mathematics/random-samples-and-the-statistics.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Random Samples

The random variables $X_1, \ldots, X_n$ are said to form a **random sample** if they are _independent and identically distributed_ (i.i.d.).
This means that each $X_i$ is drawn from the same probability distribution $f(x)$ and that the values of $X_i$ do not influence each other.
Here we call $f(x)$ the **population distribution**. From the definition, the joint distribution of the random sample is given by:

\\[
f(x_1, \ldots, x_n) = \prod_{i=1}^n f(x_i)
\\]

Provided that the mgf of the population distribution exists, the mgf of the sample mean is:

\\[
M_{\bar{X}}(t) = \left[ M_X\left(\frac{t}{n}\right) \right]^n
\\]

It appears the same for the characteristic functions.

## Statistics

A **statistic** is a function of the random sample $X_1, \ldots, X_n$ that does not depend on the parameters of the population distribution.
We can denote a statistic as $T(X_1, \ldots, X_n)$, where $T$ is a function that takes the random sample as input.
The probability distribution of a statistic is called the **sampling distribution**.
A statistic is a random variable that summarizes or describes some aspect of the sample, such as the sample mean, sample variance, or sample median.

### Sample Mean

The **sample mean** is the arithmetic average of the random sample and is usually denoted as:

\\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\\]

Assuming $\mathrm{E}[X_i] = \mu$ and $\mathrm{Var}[X_i] = \sigma^2$ for all $i$, the expected value of the sample mean is given by:

\\[
\mathrm{E}[\bar{X}] = \frac{1}{n} \sum_{i=1}^n \mathrm{E}[X_i] = \mu
\\]

The variance of the sample mean is given by:

\\[
\mathrm{Var}[\bar{X}] = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}[X_i] = \frac{\sigma^2}{n}
\\]

### Sample Variance

The **sample variance** is a measure of the spread of the random sample and is usually denoted as:

\\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n \left(X_i - \bar{X}\right)^2
\\]

The **sample standard deviation** is the square root of the sample variance.

The expected value of the sample variance is given by:

\\[
\mathrm{E}[S^2] = \sigma^2
\\]

Why do we divide by $n-1$ instead of $n$? This is because we are estimating the population variance from the sample, and dividing by $n-1$ (the **Bessel&apos;s correction**) corrects the bias in the estimation.
Let&apos;s prove this:

\\[
(n-1)S^2 = \sum_{i=1}^n (X_i - \bar{X})^2 = \sum_{i=1}^n X_i^2 - n\bar{X}^2
\\]

Using the fact that $\mathrm{E}[X^2]=\mathrm{Var}[X] + \mathrm{E}[X]^2$, we can compute the expected value:

\\[
\mathrm{E}[(n-1)S^2] = n (\sigma^2 + \mu^2) - n\left( \frac{\sigma^2}{n} + \mu^2 \right) = (n-1)\sigma^2
\\]

### Sampling Distribution of the Sample Mean

If $X$ and $Y$ are independent random variables with pdfs $f_X(x)$ and $f_Y(y)$, then the pdf of the sum $Z = X + Y$ is given by the convolution of the two pdfs.
One can prove the theorem directly from the formula derived [here](/mathematics/multivariate-transformations.html):

\\[
f_Z(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z - x) \, dx
\\]

We can extend this to the case of the sample mean.
If $X_1, \ldots, X_n$ are independent and identically distributed random variables with pdf $f(x)$, then the pdf of the sample mean $\bar{X}$ is given by:

\\[
f_{\bar{X}}(\bar{x}) = n \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} f\left(n \bar{x} - \sum_{i=1}^{n-1} x_i\right) \prod_{i=1}^{n-1} f(x_i) \, dx_1 \cdots dx_{n-1}
\\]</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="statistics" /><summary type="html"></summary></entry><entry><title type="html">Sampling from the Normal Distribution</title><link href="http://localhost:4000/mathematics/sampling-from-the-normal-distribution.html" rel="alternate" type="text/html" title="Sampling from the Normal Distribution" /><published>2025-08-05T00:00:00+09:00</published><updated>2025-08-05T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/sampling-from-the-normal-distribution</id><content type="html" xml:base="http://localhost:4000/mathematics/sampling-from-the-normal-distribution.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Statistics of the normal random sample

Let $X_1, \ldots, X_n$ be a random sample from the normal distribution with mean $\mu$ and variance $\sigma^2$,
i.e., $X_i \sim N(\mu, \sigma^2)$. Let $\bar{X}$ be the sample mean and $S^2$ be the sample variance of the random sample.
Then the following properties hold:
- $\bar{X}$ and $S^2$ are independent random variables.
- $\bar{X} \sim N(\mu, \sigma^2/n)$
- $(n-1) S^2 / \sigma^2 \sim \chi^2_{n-1}$

For the first one, one can show that the variables $\bar{X}$ and $S^2$ are independent so that the two statistics can be treated separately.
We&apos;ll show it elegantly a bit later.
The second one follows from the properties of the normal distribution, which states that the sum of independent normal random variables is also normally distributed.
The third one follows from the definition of the sample variance and the properties of the chi-squared distribution.
Let&apos;s prove it directly.

### Chi-squared distribution

The _chi-squared distribution_ with $p$ degrees of freedom is defined as the following distribution:

\\[
f_{\chi^2_p}(x) = \frac{1}{2^{p/2} \Gamma(p/2)} x^{\frac{p}{2}-1} e^{-x/2}, \quad x &gt; 0
\\]

Chi-squared distribution is also defined as the sum of the squares of $p$ independent standard normal random variables.
First, we can show that the square of a standard normal random variable is chi-squared distributed with one degree of freedom.

\\[
f_Z(z) = \frac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}
\\]

\\[
\begin{align\*}
f_{Z^2}(x) &amp;= f_Z(\sqrt{x})\frac{1}{2\sqrt{x}} + f_Z(-\sqrt{x})\frac{1}{2\sqrt{x}} \nl
&amp;= \frac{1}{\sqrt{2\pi x}} e^{-\frac{x}{2}} \nl
&amp;= f_{\chi^2_1}(x)
\end{align\*}
\\]

Now, we can show that the sum of $p$ independent chi-squared distributed random variables with one degree of freedom is chi-squared distributed with $p$ degrees of freedom,
using the moment generating function (mgf). Referring to [here](/mathematics/common-discrete-and-continuous-distributions.html#chi-squared-distribution), the mgf of the chi-squared distribution is given by:

\\[
M_{\chi^2_p}(t) = (1 - 2t)^{-p/2}, \quad t &lt; \frac{1}{2}
\\]

The mgf of the sum of independent random variables is the product of their individual mgfs.
Denote $Y = \sum_{i=1}^p Z_i^2$, where $Z_i$ are independent standard normal random variables.
Then the mgf of $Y$ is given by:

\\[
M_Y(t) = \prod_{i=1}^p M_{Z_i^2}(t) = (1 - 2t)^{-p/2} = M_{\chi^2_p}(t)
\\]

This shows that $Y$ is chi-squared distributed with $p$ degrees of freedom.

\\[
Y \sim \chi^2_p
\\]

Also, similarly we can show the following property of the chi-squared distribution:

\\[
X\sim \chi^2\_p, \; Y\sim \chi^2\_q \implies X \pm Y \sim \chi^2\_{p \pm q}
\\]

Be careful that $X$ and $Y$ must be independent for this property to hold.

### Proof of the sample variance

We know the following equation holds:

\\[
(n-1)S^2 = \sum_{i=1}^n (X_i - \bar{X})^2 = \sum_{i=1}^n (X_i - \mu)^2 - n(\bar{X} - \mu)^2
\\]

where $\mu$ is the population mean.
Then we can write as:

\\[
\frac{(n-1)S^2}{\sigma^2} = \sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2 - \left(\frac{\bar{X} - \mu}{\sigma/\sqrt{n}}\right)^2
\\]

Denote $Z_i = (X_i - \mu)/\sigma$ and $Z = (\bar{X} - \mu)/(\sigma/\sqrt{n})$.
Then we have:

\\[
\frac{(n-1)S^2}{\sigma^2} = \sum_{i=1}^n Z_i^2 - Z^2
\\]

and also we know that $Z_i \sim \mathcal{N}(0, 1)$ and $Z \sim \mathcal{N}(0, 1)$, by the properties above.
Finally, by the property of the chi-squared distribution, we have:

\\[
\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}
\\]

### Independence of $\bar{X}$ and $S^2$

Let&apos;s prove the following Lemma.

Let $X_j\sim\mathcal{N}\;(\mu_j,\sigma_j^2) (j=1,\ldots,n)$ which are mutually independent.
For constants $a_{ij}$ where $i=1,\ldots,n$, define

\\[
U_i = \sum_{j=1}^n a_{ij} X_j \;(i=1,\ldots,n)
\\]

or write as a vector form:

\\[
\mathbf{U} = A \mathbf{X}
\\]

Then, the random variables $U_i$ and $U_j$ are independent if and only if $\text{Cov}(U_i,U_j)=0$. Furtheremore,

\\[
\text{Cov}(U_i,U_j) = \sum_{j=1}^n a_{ik} a_{jk} \sigma_k^2
\\]

&lt;details markdown=&quot;1&quot;&gt;&lt;summary&gt;Proof&lt;/summary&gt;

The joint distribution of $\mathbf{X}$ is:

\\[
f_{\mathbf{X}}(\mathbf{x}) = \frac{1}{(2\pi)^{n/2}\prod_{i=1}^n \sigma_i} \exp\left(-\frac{1}{2}\sum_{i=1}^n\left\[\frac{x_i-\mu_i}{\sigma_i}\right\]^2 \right)
\\]

by defining the following constants,

\\[
\bs{\mu} = \begin{bmatrix}
\mu_1 \nl \vdots \nl \mu_n
\end{bmatrix}, \quad D = \text{diag}(\sigma_1^2, \cdots, \sigma_n^2), \quad
C = \frac{1}{(2\pi)^{n/2}\prod_{i=1}^n \sigma_i}
\\]

we can also write the distribution as:

\\[
f_{\mathbf{X}}(\mathbf{x}) = C \exp\left(-\frac{1}{2}(\mathbf{x}-\bs{\mu})^\top D^{-1} (\mathbf{x}-\bs{\mu}) \right)
\\]

Then the distribution of $\mathbf{U}$ will be:

\\[
f_{\mathbf{U}}(\mathbf{u}) = C \exp\left(-\frac{1}{2}\left(A^{-1}\mathbf{u}-\bs{\mu}\right)^\top D^{-1} \left(A^{-1}\mathbf{u}-\bs{\mu}\right) \right) \lVert A^{-1} \rVert
\\]

By defining $\bs{\mu}^\prime = A\bs{\mu}$,

\\[
\begin{align\*}
f_{\mathbf{U}}(\mathbf{u}) &amp;= C \exp\left(-\frac{1}{2}\left(A^{-1}(\mathbf{u}-\bs{\mu}^\prime)\right)^\top D^{-1} \left(A^{-1}(\mathbf{u}-\bs{\mu}^\prime)\right) \right) \frac{1}{\Vert A \Vert} \nl
&amp;= \frac{C}{\Vert A \Vert} \exp\left(-\frac{1}{2}\left(\mathbf{u}-\bs{\mu}^\prime\right)^\top \left(A^\top\right)^{-1} D^{-1} A^{-1} \left(\mathbf{u}-\bs{\mu}^\prime\right) \right) \nl
&amp;= \frac{C}{\Vert A \Vert} \exp\left(-\frac{1}{2}\left(\mathbf{u}-\bs{\mu}^\prime\right)^\top \left(ADA^\top\right)^{-1} \left(\mathbf{u}-\bs{\mu}^\prime\right) \right)
\end{align\*}
\\]

Referring to [here](/covariance-and-correlation.html#multivariate-normal-distribution), this is the pdf of the multivariate normal distribution, and
$ADA^\top$ is the covariance matrix.
Therefore, we get $\text{Cov}(U_i,U_j) = \left\[ADA^\top\right\]\_{ij} = \sum\_{j=1}^n a\_{ik} a\_{jk} \sigma\_k^2$.
And then we can induce that $\text{Cov}(U_i,U_j)=0$ implies the independence of $U_i$ and $U_j$, and also the inverse.
&lt;/details&gt;

We can transform the sample variance equation as:

\\[
\begin{align\*}
S^2 &amp;= \frac{1}{n-1} \sum_{i=1}^n \left(X_i-\bar{X}\right)^2 \nl
&amp;= \frac{1}{n-1} \left( \left\[ \sum_{i=2}^n \left(X_i-\bar{X}\right) \right\]^2 \sum_{i=2}^n \left(X_i-\bar{X}\right)^2 \right)
\end{align\*}
\\]

Therefore, $S^2$ is the function only of $\left(X_2-\bar{X},\ldots,X_n-\bar{X}\right)$.
So if we show the independence of $\bar{X}$ and $X_j-\bar{X}$s, we can show the independence of $\bar{X}$ and $S^2$.
As an illustration of the application of the lemma, write:

\\[
\begin{align\*}
\bar{X} &amp;= \sum_{i=1}^n \frac{1}{n} X_i \nl
X_j-\bar{X} &amp;= \sum_{i=1}^n \left( \delta_{ij} - \frac{1}{n} \right) X_i
\end{align\*}
\\]

It is then easy to show that 

\\[
\text{Cov}\left(\bar{X},X_j-\bar{X}\right) = \sum_{i=1}^n \frac{1}{n}\left(\delta_{ij}-\frac{1}{n}\right) \sigma^2 = 0
\\]

as long as the $X_i$s have the same variance since it&apos;s from the identical population. Thus, the proof is end.

## Student&apos;s t-distribution

If $X_1,\ldots,X_n$ are a random sample from a $\mathcal{N}(\mu,\sigma^2)$, we know that the quantity

\\[
\frac{\bar{X}-\mu}{\sigma\sqrt{n}}
\\]

is distributed as a $\mathcal{N}(0,1)$ random variable. However, in most cases, the value of $\sigma$ is unknown so that we should use the sample standard deviation $S$.
Therefore, we should investigate the distribution of the following value:

\\[
\frac{\bar{X}-\mu}{S\sqrt{n}}
\\]

We can write it in a slightly different way.

\\[
\frac{\left(\bar{X}-\mu\right)/(\sigma/\sqrt{n})}{\sqrt{S^2/\sigma^2}}
\\]

Here, the numerator is a $\mathcal{N}(0,1)$ random variable, and the denominator is $\sqrt{\chi^2_{n-1}/(n-1)}$ random variable, and we can infer that these
two random variables are independent since $\bar{X}$ and $S^2$ are independent which is proved above. Thus, we should find the distribution of:

\\[
T = \frac{Z}{\sqrt{V/\nu}}
\\]

where $Z$ is the standard normal random variable and $V$ is the chi-squared random variable with $\nu$ degrees of freedom.
And the distribution of this new random variable $T$ is called the **student&apos;s t-distribution** with $\nu$ degrees of freedom,
or simply the **t-distribution**.
Now let&apos;s derive the actual distribution.

\\[
f_{U,V}(u,v) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2} \frac{1}{\Gamma\left(\frac{\nu}{2}\right) 2^{\nu/2}} v^{\frac{\nu}{2}-1} e^{-v/2}
\\]

Now make the transformation

\\[
G(u,v) = \left( \frac{u}{\sqrt{v/\nu}}, v \right) = (t,w)
\\]

Therefore,

\\[
\begin{align\*}
f_T(t) &amp;= \int_0^\infty f_{U,V} \left(t\sqrt{\frac{w}{\nu}} ,w \right) \abs{ \pdv{G^{-1}}{(t,w)} } \,\dd{w} \nl
&amp;= \frac{1}{\sqrt{2\pi}} \frac{1}{\Gamma\left(\frac{\nu}{2}\right) 2^{\nu/2}} \int_0^\infty \exp\left( -\frac{t^2w}{2\nu}\right) w^{\frac{\nu}{2}-1} e^{-w/2} \sqrt{\frac{w}{\nu}} \, \dd{w} \nl
&amp;= \frac{1}{\sqrt{2\pi\nu}\Gamma\left(\frac{\nu}{2}\right) 2^{\nu/2}} \int_0^\infty \exp\left( -\frac{1}{2}\left(1+\frac{t^2}{\nu}\right) w \right) w^{\frac{\nu+1}{2}-1} \,\dd{w} \nl
&amp;= \frac{1}{\sqrt{2\pi\nu}\Gamma\left(\frac{\nu}{2}\right) 2^{\nu/2}} \Gamma\left( \frac{\nu+1}{2} \right) \left\[ \frac{1+t^2/\nu}{2} \right\]^{-(\nu+1)/2} \nl
&amp;= \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\pi\nu}\Gamma\left(\frac{\nu}{2}\right)} \left( 1+\frac{t^2}{\nu} \right)^{-(\nu+1)/2}
\end{align\*}
\\]

This is the PDF of the student&apos;s t-distribution with $\nu$ degrees of freedom:

\\[
f_T(t) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\pi\nu}\Gamma\left(\frac{\nu}{2}\right)} \left( 1+\frac{t^2}{\nu} \right)^{-(\nu+1)/2}
\\]

and we know that:

\\[
\frac{\bar{X}-\mu}{S\sqrt{n}} \sim t_{n-1}
\\]

Let&apos;s seek for the properties of the student&apos;s t-distribution.

| Parameters | $\supp f_T$ (Support) | $f_T(x)$ (PDF)                                                                                                                                | $F_T(x)$ (CDF)                                                                                                                                                                                                                      | $\mathrm{E}[T]$ (Mean)                                                  | $\mathrm{Var}[T]$ (Variance)                                                                                       | $\gamma_1$ (Skewness)                                                   | $\gamma_2$ (Kurtosis)                                                                                            | $M_T(t)$ (MGF) | $\phi_T(t)$ (CF)                                                                                                    |
|:-----------|:----------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:---------------|:--------------------------------------------------------------------------------------------------------------------|
| $\nu &gt; 0$  | $\mathbb{R}$          | $\dfrac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\; \Gamma\left(\frac{\nu}{2}\right)} \left(1 + \dfrac{x^2}{\nu}\right)^{-(\nu+1)/2}$ | $\dfrac{1}{2} + x \dfrac{\Gamma\left(\frac{\nu+1}{2}\right) }{\sqrt{\pi\nu} \Gamma\left(\frac{\nu}{2}\right) } {}_2F_1\left\[ \begin{matrix} \frac{1}{2}, \frac{\nu+1}{2} \nl \frac{3}{2} \end{matrix} ; -\frac{x^2}{\nu} \right\]$ | $\begin{cases} 0 &amp;; \nu&gt;1 \nl \text{undefined} &amp;; \nu\le 1 \end{cases}$ | $\begin{cases} \dfrac{\nu}{\nu-2} &amp;; \nu&gt;2 \nl \infty &amp;; 1&lt;\nu\le 2 \nl \text{undefined} &amp;; \nu \le 1 \end{cases}$ | $\begin{cases} 0 &amp;; \nu&gt;3 \nl \text{undefined} &amp;; \nu\le 3 \end{cases}$ | $\begin{cases} \dfrac{6}{\nu-4} &amp;; \nu&gt;4 \nl \infty &amp;; 2&lt;\nu\le 4 \nl \text{undefined} &amp;; \nu \le 2 \end{cases}$ | undefined      | $\dfrac{ (\sqrt{\nu}\abs{t})^{\nu/2} K_{\nu/2}(\sqrt{\nu}\abs{t})}{\Gamma\left(\frac{\nu}{2}\right) 2^{\nu/2 - 1}}$ |
{:.scroll-table}

## Snedecor&apos;s F-distribution

Let $X_1,\ldots,X_n$ be a random sample from a $\mathcal{N}(\mu_X,\sigma_X^2)$ population, and let
$Y_1,\ldots,Y_m$ be a random sample from a $\mathcal{N}(\mu_Y,\sigma_Y^2)$ population.
If we are interested in comparing the variability of the populations, one quantity of interest woul be the ratio $\sigma_X^2/\sigma_Y^2$.
Information about this ratio is contained in $S_X^2/S_Y^2$, the ratio of sample variances. So, we should investigate the distribution of the following value:

\\[
\frac{S_X^2/S_Y^2}{\sigma_X^2/\sigma_Y^2} = \frac{S_X^2/\sigma_X^2}{S_Y^2/\sigma_Y^2}
\\]

We can observe that the numerator and the denominator of the rhs are both proportional to the chi-squared random variable.
Thus, we should find the distribution of:

\\[
F = \frac{U/p}{V/q}
\\]

where $U\sim\chi^2_p$ and $V\sim\chi^2_q$. The distribution of this new random variable is called the **Snedecor&apos;s F-distribution** with $p$ and $q$ degrees of freedom,
or simply the **F-distribution**.   

\\[
f_{U,V}(u,v) = \frac{1}{\Gamma\left(\frac{p}{2}\right) 2^{p/2}} u^{\frac{p}{2}-1} e^{-u/2} \cdot \frac{1}{\Gamma\left(\frac{q}{2}\right) 2^{q/2}} v^{\frac{q}{2}-1} e^{-v/2}
\\]

Now make the transformation

\\[
G(u,v) = \left( \frac{u/p}{v/q}, v \right) = (x,w)
\\]

Therefore,

\\[
\begin{align\*}
f_F(x) &amp;= \int_0^\infty f_{U,V}\left( \frac{pxw}{q}, w \right) \abs{ \pdv{G^{-1}}{(x,w)} } \,\dd{w} \nl
&amp;= \frac{1}{\Gamma\left(\frac{p}{2}\right) 2^{p/2} \Gamma\left(\frac{q}{2}\right) 2^{q/2}} \int_0^\infty \left( \frac{pxw}{q} \right)^{\frac{p}{2}-1} e^{-pxw/2q} w^{\frac{q}{2}-1} e^{-w/2} \cdot \frac{pw}{q} \,\dd{w} \nl
&amp;= \frac{1}{\Gamma\left(\frac{p}{2}\right) \Gamma\left(\frac{q}{2}\right) 2^{(p+q)/2}} \left( \frac{p}{q} \right)^{p/2} x^{\frac{p}{2}-1} \int_0^\infty w^{\frac{p+q}{2}-1} e^{-\frac{1}{2}\left(1 + \frac{px}{q} \right) w} \,\dd{w} \nl
&amp;= \frac{1}{\Gamma\left(\frac{p}{2}\right) \Gamma\left(\frac{q}{2}\right) 2^{(p+q)/2}} \left( \frac{p}{q} \right)^{p/2} x^{\frac{p}{2}-1} \Gamma\left(\frac{p+q}{2} \right) \left( \frac{1}{2}\left(1 + \frac{px}{q} \right) \right)^{-(p+q)/2} \nl
&amp;= \frac{\Gamma\left( \frac{p+q}{2} \right)}{\Gamma\left( \frac{p}{2} \right)\Gamma\left( \frac{q}{2} \right)} \left( \frac{p}{q} \right)^{p/2} x^{\frac{p}{2}-1} \left(1 + \frac{px}{q} \right)^{-(p+q)/2} \nl
&amp;= \frac{1}{\mathrm{B}\left( \frac{p}{2},\frac{q}{2} \right)} \left( \frac{p}{q} \right)^{p/2} x^{\frac{p}{2}-1} \left(1 + \frac{p}{q}x \right)^{-(p+q)/2}
\end{align\*}
\\]

This is the PDF of the F-distribution with $p$ and $q$ degrees of freedom:

\\[
f_F(x) = \frac{1}{\mathrm{B}\left( \frac{p}{2},\frac{q}{2} \right)} \left( \frac{p}{q} \right)^{p/2} x^{\frac{p}{2}-1} \left(1 + \frac{p}{q}x \right)^{-(p+q)/2}
\\]

and we know that:

\\[
\frac{S_X^2/S_Y^2}{\sigma_X^2/\sigma_Y^2} \sim F_{p,q}
\\]

Let’s seek for the properties of the F-distribution.

| Parameters | $\supp f_F$ (Support)                                                             | $f_F(x)$ (PDF)                                                                                                                                               | $F_F(x)$ (CDF)                                                 | $\mathrm{E}[F]$ (Mean)                                                           | $\mathrm{Var}[F]$ (Variance)                                                                            | $\gamma_1$ (Skewness)                                                                                                    | $\gamma_2$ (Kurtosis)                                                                                                         | $M_F(t)$ (MGF) | $\phi_F(t)$ (CF)                                                                                                                           |
|:-----------|:----------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------|:---------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|:---------------|:-------------------------------------------------------------------------------------------------------------------------------------------|
| $p,q &gt; 0$  | $\begin{cases} \mathbb{R}\_{\ge0} &amp;; p\ge 2 \nl \mathbb{R}\_+ &amp;; p&lt;2 \end{cases}$ | $\dfrac{1}{\mathrm{B}\left( \frac{p}{2},\frac{q}{2} \right)} \left( \dfrac{p}{q} \right)^{p/2} x^{\frac{p}{2}-1} \left(1 + \dfrac{p}{q}x \right)^{-(p+q)/2}$ | $I_{\frac{px}{px+q}} \left(\dfrac{p}{2}, \dfrac{q}{2} \right)$ | $\begin{cases} \dfrac{q}{q-2} &amp;; q&gt;2 \nl \text{undefined} &amp;; q\le 2 \end{cases}$ | $\begin{cases} \dfrac{2q^2 (p+q-2)}{p(q-2)^2 (q-4)} &amp;; q&gt;4 \nl \text{undefined} &amp;; q \le 4 \end{cases}$ | $\begin{cases} \dfrac{(2p+q- 2) \sqrt{8(q-4)}}{(q-6) \sqrt{p(p+q-2)}} &amp;; q&gt;6 \nl \text{undefined} &amp;; q\le 6 \end{cases}$ | $\begin{cases} \dfrac{12p(5q-22)(p+q-2)+(q-4)(q-2)^2}{p(q-6)(q-8)(p+q-2)} &amp;; q&gt;8 \nl \text{undefined} &amp;; q \le 8 \end{cases}$ | undefined      | $\dfrac{\Gamma{\left(\frac{p+q}{2}\right)}}{\Gamma{\left(\frac{q}{2}\right)}} U \left(\dfrac{p}{2},1-\dfrac{q}{2},-\frac{q}{p} it \right)$ |
{:.scroll-table}

- $X\sim F_{p,q} \implies 1/X \sim F_{q,p}$
- $X\sim t_q \implies X^2 \sim F_{1,q}$
- $X\sim F_{p,q} \implies pX/(q+pX) \sim \mathrm{Beta}(p/2,q/2)$</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="statistics" /><summary type="html"></summary></entry><entry><title type="html">Probabilistic Inequalities</title><link href="http://localhost:4000/mathematics/probabilistic-inequalities.html" rel="alternate" type="text/html" title="Probabilistic Inequalities" /><published>2025-08-03T00:00:00+09:00</published><updated>2025-08-03T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/probabilistic-inequalities</id><content type="html" xml:base="http://localhost:4000/mathematics/probabilistic-inequalities.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Hölder&apos;s inequality

**Hölder&apos;s inequality** states that for any random variables $X$ and $Y$ and for any $p, q &gt; 1$ such that

\\[
\frac{1}{p} + \frac{1}{q} = 1
\\]

the following holds:

\\[
\abs{\mathrm{E}[XY]} \leq \mathrm{E}\abs{XY} \leq \big(\mathrm{E}\abs{X}^p\big)^\frac{1}{p} \big(\mathrm{E}\abs{Y}^q\big)^\frac{1}{q}
\\]

### Lemma

Let $a$ and $b$ be non-negative real numbers. Then we have:

\\[
\frac{1}{p} a^p + \frac{1}{q} b^q \geq a b
\\]

with equality if and only if $a^p = b^q$.

&lt;details markdown=&quot;1&quot;&gt;&lt;summary&gt; Proof&lt;/summary&gt;

Fix $b$, and consider the function

\\[
f(a) = \frac{1}{p} a^p + \frac{1}{q} b^q - ab
\\]

The derivative is given by:
\\[
f\&apos;(a) = a^{p-1} - b
\\]

To minimize $f(a)$, we set $f\&apos;(a) = 0$ and get $a = b^{\frac{1}{p-1}} = b^{\frac{q}{p}}$. Substituting this back into $f(a)$ gives:

\\[
f\left(b^{\frac{q}{p}}\right) = \frac{1}{p} b^q + \frac{1}{q} b^q - b^{\frac{q}{p}+1} = b^q - b^q = 0
\\]

A check of the second derivative confirms that this is indeed a minimum, and thus we prove the lemma.
&lt;/details&gt;

### Proof

The first inequality simply follows from $-\abs{XY} \le XY \le \abs{XY}$. Now let&apos;s prove the second one. By setting

\\[
a = \frac{\abs{X}}{(\mathrm{E}\abs{X}^p)^{\frac{1}{p}}}, \quad b = \frac{\abs{Y}}{(\mathrm{E}\abs{Y}^q)^{\frac{1}{q}}}
\\]

and applying the lemma, we have:

\\[
\frac{1}{p} \frac{\abs{X}^p}{\mathrm{E}\abs{X}^p} + \frac{1}{q} \frac{\abs{Y}^q}{\mathrm{E}\abs{Y}^q} \geq \frac{\abs{XY}}{(\mathrm{E}\abs{X}^p)^{\frac{1}{p}} (\mathrm{E}\abs{Y}^q)^{\frac{1}{q}}}
\\]

Taking expectation on both sides yields:

\\[
1 \geq \frac{\mathrm{E}\abs{XY}}{(\mathrm{E}\abs{X}^p)^{\frac{1}{p}} (\mathrm{E}\abs{Y}^q)^{\frac{1}{q}} }
\\]

and thus we obtain Hölder&apos;s inequality.

### Cauchy--Schwarz inequality

**Cauchy--Schwarz inequality** is a special case of Hölder&apos;s inequality where $p = q = 2$. It states that for any random variables $X$ and $Y$:

\\[
\abs{\mathrm{E}[XY]} \leq \mathrm{E}\abs{XY} \leq \sqrt{\mathrm{E}\abs{X}^2} \sqrt{\mathrm{E}\abs{Y}^2}
\\]

or also written as:

\\[
(\mathrm{E}[XY])^2 \leq \mathrm{E}\left[X^2\right] \mathrm{E}\left[Y^2\right]
\\]

### Covariance and correlation

If $X$ and $Y$ have means $\mu_X, \mu_Y$ and variances $\sigma_X^2, \sigma_Y^2$, we can apply Cauchy--Schwarz inequality to the centered variables $X - \mu_X$ and $Y - \mu_Y$:

\\[
\abs{\mathrm{E}[(X - \mu_X)(Y - \mu_Y)]} \leq \sqrt{\mathrm{E}[(X - \mu_X)^2]} \sqrt{\mathrm{E}[(Y - \mu_Y)^2]}
\\]

This leads to the well-known properties of covariance and correlation:

\\[
\abs{\text{Cov}(X, Y)} \leq \sigma_X \sigma_Y
\\]

and

\\[
\abs{\text{Corr}(X, Y)} \leq 1
\\]

## Minkowski&apos;s inequality

**Minkowski&apos;s inequality** is a generalization of the triangle inequality for $L^p$ norms. It states that for any random variables $X$ and $Y$ and for any $p \geq 1$:

\\[
(\mathrm{E}\abs{X+Y}^p)^{\frac{1}{p}} \leq (\mathrm{E}\abs{X}^p)^{\frac{1}{p}} + (\mathrm{E}\abs{Y}^p)^{\frac{1}{p}}
\\]

Proof is omitted. It can be done by applying Hölder&apos;s inequality and the basic triangle inequality $\abs{X+Y} \leq \abs{X} + \abs{Y}$.

## Jensen&apos;s inequality

**Jensen&apos;s inequality** states that for any convex function $g$ and any random variable $X$:

\\[
E[g(X)] \geq g(E[X])
\\]

Equality holds if and only if $g$ is linear on the support of $X$. If $g$ is concave, the inequality is reversed:

\\[
E[g(X)] \leq g(E[X])
\\]

### Proof

Let $l(x)$ be the linear function that is tangent to $g$ at the point $x= E[X]$. Then, by the definition of convexity, we have $g(x) \geq l(x)$ for all $x$. Taking expectation on both sides gives:

\\[
\mathrm{E}[g(X)] \geq \mathrm{E}[l(X)] = l(\mathrm{E}[X]) = g(\mathrm{E}[X])
\\]

## Inequality for means

Jensen&apos;s inequality can be used to prove an inequality between the three means: arithmetic mean, geometric mean, and harmonic mean.
We define the **arithmetic mean** of a positive random variable $X$ as:

\\[
\mathrm{AM}[X] = \mathrm{E}[X]
\\]

the **geometric mean** as:

\\[
\mathrm{GM}[X] = \exp\left(\mathrm{E}[\ln X]\right)
\\]

and the **harmonic mean** as:
\\[
\mathrm{HM}[X] = \frac{1}{\mathrm{E}[1/X]}
\\]

Then $\mathrm{AM}[X] \geq \mathrm{GM}[X] \geq \mathrm{HM}[X]$ for any positive random variable $X$.
More generally, for any $p\in \mathbb{R}$, we define the **power mean** as:

\\[
M_p[X] = \left(\mathrm{E}[X^p]\right)^{\frac{1}{p}} = \exp\left(\frac{1}{p} \ln \mathrm{E}[X^p]\right)
\\]

We can check that $M_1[X] = \mathrm{AM}[X]$, $M_0[X] = \mathrm{GM}[X]$, and $M_{-1}[X] = \mathrm{HM}[X]$.
Also, for $p\to \infty$, we have $M_p[X] \to \max X$ and for $p\to -\infty$, we have $M_p[X] \to \min X$.
Checking this later, we have the powerful theorem that $M_p[X]$ is an increasing function of $p$ for any positive random variable $X$.

\\[
\pdv{}{p} M_p[X] \geq 0
\\]

### Special values of $p$

Let&apos;s check that $M_0[X] = \mathrm{GM}[X]$ is indeed the geometric mean.
By the L&apos;Hôpital&apos;s rule, we have:

\\[
\lim_{p \to 0} \ln M_p[X] = \lim_{p \to 0} \frac{\ln \mathrm{E}[X^p]}{p} = \lim_{p \to 0} \frac{\mathrm{E}[ X^p \ln X]}{\mathrm{E}[X^p]} = \mathrm{E}[\ln X]
\\]

Thus, we have:

\\[
\lim_{p \to 0} M_p[X] = \exp\left(\mathrm{E}[\ln X]\right) = \mathrm{GM}[X]
\\]

Let&apos;s also check that $M_\infty[X] = \max X$ is indeed the maximum.
We can write the power mean as:

\\[
M_p[X] = \max X \exp\left(\frac{1}{p} \ln \mathrm{E}\left[\left(\frac{X}{\max X}\right)^p\right]\right)
\\]

Also by the L&apos;Hôpital&apos;s rule, we have:

\\[
\begin{align\*}
\lim_{p \to \infty} \ln \frac{M_p[X]}{\max X} &amp;= \lim_{p \to \infty} \frac{\ln \mathrm{E}\left[\left(\frac{X}{\max X}\right)^p\right]}{p} \nl
&amp;= \lim_{p \to \infty} \frac{\mathrm{E}\left[\left(\frac{X}{\max X}\right)^p \ln \frac{X}{\max X}\right]}{\mathrm{E}\left[\left(\frac{X}{\max X}\right)^p\right]} \nl
&amp;= \frac{f_X(\max X) \ln 1}{f_X(\max X)} \nl
&amp;= 0
\end{align\*}
\\]

Thus, we have:

\\[
\lim_{p \to \infty} M_p[X] = \max X
\\]

Similarly, we can check that $M_{-\infty}[X] = \min X$ is indeed the minimum.

### Proof

First, we have the following property of the power means:

\\[
M_p[X] \geq M_q[X] \implies M_{-p}[X] \leq M_{-q}[X]
\\]

It is proved by substituting $X$ with $1/X$ in the definition of the power mean.
Now, we can prove the inequality including the geometric mean:

\\[
M_{-p}[X] \leq M_0[X] \leq M_p[X]
\\]

for any $p&gt; 0$. The proof follows from the Jensen&apos;s inequality applied to the concave function $g(x) = \ln x$:

\\[
\mathrm{E}[\ln X] \leq \ln \mathrm{E}[X]
\\]

Substituting $X$ with $X^p$ gives:

\\[
\exp \mathrm{E} [\ln X] \leq \mathrm{E}[X^p]^{\frac{1}{p}}
\\]

and substituting $X$ with $1/X$ gives:

\\[
\exp \mathrm{E} [\ln X] \geq \mathrm{E}[X^{-p}]^{-\frac{1}{p}}
\\]

and the inequality is proved. Showing that $M_p[X]$ is an increasing function at $p&gt;0$, the theorem is automatically proved by the properties proven above.
For $0&lt;p\leq q$, define $r=q/p$ and apply the Jensen&apos;s inequality to the convex function $g(x) = x^r$:

\\[
\mathrm{E}[X]^r \leq \mathrm{E}[X^r]
\\]

Substituting $Y = X^{1/p}$ gives:

\\[
\mathrm{E}[Y^p]^\frac{1}{p} \leq \mathrm{E}[Y^q]^\frac{1}{q}
\\]

Thus, we have:

\\[
p&lt;q \implies M_p[X] \leq M_q[X]
\\]

By constructing the probability theory upon the measure theory, we can treat these probabilistic properties more rigorously.</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="statistics" /><summary type="html"></summary></entry><entry><title type="html">Extreme Values of Functions</title><link href="http://localhost:4000/mathematics/extreme-values-of-functions.html" rel="alternate" type="text/html" title="Extreme Values of Functions" /><published>2025-08-01T00:00:00+09:00</published><updated>2025-08-01T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/extreme-values-of-functions</id><content type="html" xml:base="http://localhost:4000/mathematics/extreme-values-of-functions.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><summary type="html"></summary></entry><entry><title type="html">Covariance and Correlation</title><link href="http://localhost:4000/mathematics/covariance-and-correlation.html" rel="alternate" type="text/html" title="Covariance and Correlation" /><published>2025-07-29T00:00:00+09:00</published><updated>2025-07-29T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/covariance-and-correlation</id><content type="html" xml:base="http://localhost:4000/mathematics/covariance-and-correlation.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Covariance

**Covariance** is a measure of how much two random variables change together.
The covariance between two random variables $X$ and $Y$ is defined as:

\\[
\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]
\\]

### Properties of Covariance

By expanding the expectation and denoting the means as $\mu_X = \mathbb{E}[X]$ and $\mu_Y = \mathbb{E}[Y]$, we can rewrite the covariance as:

\\[
\text{Cov}(X, Y) = \mathbb{E}[XY] - \mu_X \mu_Y
\\]

Also, covariance has the following properties:

\\[
\mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2ab \text{Cov}(X, Y)
\\]

If $X$ and $Y$ are independent, then:

\\[
\text{Cov}(X, Y) = 0
\\]

However, be cautious: a covariance of zero does not imply independence of $X$ and $Y$.

## Correlation

**Correlation** is a standardized measure of the relationship between two random variables, defined as:

\\[
\rho_{X,Y} = \text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \text{Var}(Y)}}
\\]

By denoting the standard deviations as $\sigma_X = \sqrt{\text{Var}(X)}$ and $\sigma_Y = \sqrt{\text{Var}(Y)}$, we can express correlation as:

\\[
\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
\\]

### Properties of Correlation

If $X$ and $Y$ are independent, then:

\\[
\rho_{X,Y} = \frac{0}{\sigma_X \sigma_Y} = 0
\\]

Also, correlation has the following important property:
- $-1 \leq \rho_{X,Y} \leq 1$
- $\abs{\rho_{X,Y}} = 1$ if and only if there exists $a\neq 0$ such that $Y = aX + b$ for some constant $b$. Then, $\rho_{X,Y} = \text{sgn}\,a$.

&lt;details markdown=&quot;1&quot;&gt;&lt;summary&gt;Proof&lt;/summary&gt;
\\[
\begin{align\*}
h(t) &amp;:= \mathrm{E}\left[ ((X-\mu_X)t+(Y-\mu_Y))^2  \right] \nl
&amp;= t^2 \sigma_X^2 + 2t \text{Cov}(X, Y) + \sigma_Y^2 \geq 0 \nl
\Rightarrow \;\; \text{Disc}_t h &amp;= 4 \text{Cov}(X, Y)^2 - 4 \sigma_X^2 \sigma_Y^2 \leq 0 \nl
\therefore &amp; -1 \leq \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} \leq 1
\end{align\*}
\\]

The equality holds if and only if the discriminant is zero, which means $h(t)$ is a perfect square. This implies that there exists a constant $a \neq 0$ such that $Y = aX + b$ for some constant $b$, leading to $\rho_{X,Y} = \text{sgn}\, a$.
&lt;/details&gt;

## Multivariate Normal Distribution

The **multivariate normal distribution** is a generalization of the normal distribution to multiple dimensions. A random vector $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ follows a multivariate normal distribution if every linear combination of its components is normally distributed.
The multivariate normal distribution is characterized by its mean vector $\bs{\mu} = (\mu_1, \mu_2, \ldots, \mu_n)$ and covariance matrix $\bs{\Sigma}$, which is a symmetric positive semi-definite matrix.

The probability density function of a multivariate normal distribution is given by:

\\[
f(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^n \det \bs{\Sigma}}} \exp\left(-\frac{1}{2}(\mathbf{x} - \bs{\mu})^\top \bs{\Sigma}^{-1} (\mathbf{x} - \bs{\mu})\right)
\\]

where $\mathbf{x}$ is a vector in $\mathbb{R}^n$, $\bs{\mu}$ is the mean vector, and $\bs{\Sigma}$ is the covariance matrix: $\Sigma_{i,j} = \text{Cov}(X_i, X_j)$.
Then we write as:

\\[
\mathbf{X} \sim \mathcal{N}(\bs{\mu}, \Sigma)
\\]

Let&apos;s check an important property of the multivariate normal distribution:

\\[
X_i \sim \mathcal{N}(\mu_i, \sigma_i^2)
\\]

where $\sigma_i^2 = \Sigma_{i,i}$ is the variance of $X_i$ and $\mu_i$ is the mean of $X_i$.</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="statistics" /><summary type="html"></summary></entry></feed>