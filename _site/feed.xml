<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-07-28T04:56:33+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jiho’s Blog</title><subtitle>noting down my thoughts and experiences
</subtitle><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><entry><title type="html">Linearization and Differentials</title><link href="http://localhost:4000/mathematics/linearization-and-differentials.html" rel="alternate" type="text/html" title="Linearization and Differentials" /><published>2025-07-28T00:00:00+09:00</published><updated>2025-07-28T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/linearization-and-differentials</id><content type="html" xml:base="http://localhost:4000/mathematics/linearization-and-differentials.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><summary type="html"></summary></entry><entry><title type="html">Conditional Distributions and Independence</title><link href="http://localhost:4000/mathematics/conditional-distributions-and-independence.html" rel="alternate" type="text/html" title="Conditional Distributions and Independence" /><published>2025-07-27T00:00:00+09:00</published><updated>2025-07-27T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/conditional-distributions-and-independence</id><content type="html" xml:base="http://localhost:4000/mathematics/conditional-distributions-and-independence.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Conditional Distribution

The **conditional distribution** of a random variable given another random variable describes the distribution of the first variable when the second variable is fixed at a certain value.
Let $(X, Y)$ be a bivariate random variable with joint distribution $f_{X,Y}(x,y)$ and marginal distributions $f_X(x)$ and $f_Y(y)$.
For any $y$ in the support of $Y$, the conditional distribution of $X$ given $Y = y$ is defined as:

\\[
f_{X\|Y}(x\|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}
\\]

This is the conditional probability mass function (PMF) for discrete random variables or the conditional probability density function (PDF) for continuous random variables.

For discrete random variables:
\\[
\sum_{x} f_{X\|Y}(x\|y) = 1
\\]
For continuous random variables:
\\[
\int_{-\infty}^{\infty} f_{X\|Y}(x\|y) \, \dd{x} = 1
\\]

## Independence

Two random variables $X$ and $Y$ are said to be **independent** if the occurrence of one does not affect the distribution of the other.
This means that the joint distribution can be expressed as the product of the marginal distributions:

\\[
f_{X,Y}(x,y) = f_X(x) f_Y(y)
\\]

If $X$ and $Y$ are independent, then the conditional distribution is equal to the marginal distribution:
* $f_{X\|Y}(x\|y) = f_X(x)$
* $f_{Y\|X}(y\|x) = f_Y(y)$

## Properties

If $X$ and $Y$ are independent, then:
- $P(X\in A, Y \in B) = P(X \in A) P(Y \in B)$ for any sets $A$ and $B$.
- $\mathrm{E}[g(X) h(Y)] = \mathrm{E}[g(X)] \mathrm{E}[h(Y)]$ for any functions $g$ and $h$.

If they have moment generating functions, respectively $M_X(t)$ and $M_Y(s)$, then $Z=X+Y$ has the moment generating function:
\\[
M_Z(t) = M_X(t) M_Y(t)
\\]

Let $X_1\sim\mathcal{N}(\mu_1, \sigma_1^2)$ and $X_2\sim\mathcal{N}(\mu_2, \sigma_2^2)$ be independent normal random variables.
Recall that the moment generating function of a normal random variable $X\sim\mathcal{N}(\mu, \sigma^2)$ is given by $M_X(t) = \exp(\mu t + \sigma^2 t^2 / 2)$.
Then by the theorem above, $Z = X_1 + X_2$ is also normally distributed:

\\[
Z \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
\\]</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="statistics" /><summary type="html"></summary></entry><entry><title type="html">Convex Hull Algorithm</title><link href="http://localhost:4000/computer-science/convex-hull-algorithm.html" rel="alternate" type="text/html" title="Convex Hull Algorithm" /><published>2025-07-26T00:00:00+09:00</published><updated>2025-07-26T00:00:00+09:00</updated><id>http://localhost:4000/computer-science/convex-hull-algorithm</id><content type="html" xml:base="http://localhost:4000/computer-science/convex-hull-algorithm.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Introduction

The convex hull of a shape is the smallest convex set that contains the shape.
It can be visualized as the shape formed by stretching a rubber band around the outermost points of the shape.
The convex hull is a fundamental concept in computational geometry and has applications in various fields such as computer graphics, pattern recognition, and geographic information systems.
Here, we will explore the **convex hull algorithm**, which is used to compute the convex hull of a set of finite points in the plane.

## Explanation</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="computer-science" /><category term="algorithms" /><summary type="html"></summary></entry><entry><title type="html">[BOJ 28399] 황혼</title><link href="http://localhost:4000/computer-science/boj28399.html" rel="alternate" type="text/html" title="[BOJ 28399] 황혼" /><published>2025-07-26T00:00:00+09:00</published><updated>2025-07-26T00:00:00+09:00</updated><id>http://localhost:4000/computer-science/boj28399</id><content type="html" xml:base="http://localhost:4000/computer-science/boj28399.html">[https://www.acmicpc.net/problem/28399](https://www.acmicpc.net/problem/28399)

&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="computer-science" /><category term="problem-solving" /><summary type="html">https://www.acmicpc.net/problem/28399</summary></entry><entry><title type="html">[BOJ 1865] 웜홀</title><link href="http://localhost:4000/computer-science/boj1865.html" rel="alternate" type="text/html" title="[BOJ 1865] 웜홀" /><published>2025-07-26T00:00:00+09:00</published><updated>2025-07-26T00:00:00+09:00</updated><id>http://localhost:4000/computer-science/boj1865</id><content type="html" xml:base="http://localhost:4000/computer-science/boj1865.html">[https://www.acmicpc.net/problem/1865](https://www.acmicpc.net/problem/1865)

&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="computer-science" /><category term="problem-solving" /><summary type="html">https://www.acmicpc.net/problem/1865</summary></entry><entry><title type="html">[BOJ 1708] 볼록 껍질</title><link href="http://localhost:4000/computer-science/boj1708.html" rel="alternate" type="text/html" title="[BOJ 1708] 볼록 껍질" /><published>2025-07-26T00:00:00+09:00</published><updated>2025-07-26T00:00:00+09:00</updated><id>http://localhost:4000/computer-science/boj1708</id><content type="html" xml:base="http://localhost:4000/computer-science/boj1708.html">[https://www.acmicpc.net/problem/1708](https://www.acmicpc.net/problem/1708)

&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="computer-science" /><category term="problem-solving" /><summary type="html">https://www.acmicpc.net/problem/1708</summary></entry><entry><title type="html">Joint and Marginal Distributions</title><link href="http://localhost:4000/mathematics/joint-and-marginal-distributions.html" rel="alternate" type="text/html" title="Joint and Marginal Distributions" /><published>2025-07-24T00:00:00+09:00</published><updated>2025-07-24T00:00:00+09:00</updated><id>http://localhost:4000/mathematics/joint-and-marginal-distributions</id><content type="html" xml:base="http://localhost:4000/mathematics/joint-and-marginal-distributions.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Multidimensional Random Variables

A **multidimensional random variable** is a vector of random variables, each representing a different dimension or aspect of the data.
An *$n$-dimensional random variable* is a vector of $n$ random variables, or interpreted as a function from a sample space to $\mathbb{R}^n$.

## Joint Distribution

The **joint distribution** of a multidimensional random variable is the probability distribution that describes
the likelihood of different combinations of values for the random variables.

If $X_1, \ldots, X_n$ are discrete random variables, the joint probability mass function (PMF) is defined as:

\\[
f_{X_1, \ldots, X_n}(x_1, \ldots, x_n) = P(X_1 = x_1, \ldots, X_n = x_n)
\\]

It has to satisfy:
\\[
\sum_{x_1, \ldots, x_n} f_{X_1, \ldots, X_n}(x_1, \ldots, x_n) = 1
\\]

For random variables $X_1, \ldots, X_n$, the joint cumulative distribution function (CDF) is defined as:

\\[
F_{X_1, \ldots, X_n}(x_1, \ldots, x_n) = P(X_1 \leq x_1, \ldots, X_n \leq x_n)
\\]

If $X_1, \ldots, X_n$ are continuous random variables, the joint probability density function (PDF) is defined as:

\\[
f_{X_1, \ldots, X_n}(x_1, \ldots, x_n) = \frac{\partial^n F_{X_1, \ldots, X_n}(x_1, \ldots, x_n)}{\partial x_1 \cdots \partial x_n}
\\]

It has to satisfy:
\\[
\int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} f_{X_1, \ldots, X_n}(x_1, \ldots, x_n) \, \dd{x_1} \cdots \dd{x_n} = 1
\\]

## Marginal Distribution

The **marginal distribution** of a random variable is the distribution of that variable alone, ignoring the others.

For discrete random variables, the marginal PMF is obtained by summing over the other variables:

\\[
f_{X_i}(x_i) = \sum_{x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n} f_{X_1, \ldots, X_n}(x_1, \ldots, x_n)
\\]

For continuous random variables, the marginal PDF is obtained by integrating over the other variables:

\\[
f_{X_i}(x_i) = \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} f_{X_1, \ldots, X_n}(x_1, \ldots, x_n) \, \dd{x_1} \cdots \dd{x_{i-1}} \dd{x_{i+1}} \cdots \dd{x_n}
\\]</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="mathematics" /><category term="statistics" /><summary type="html"></summary></entry><entry><title type="html">[BOJ 11404] 플로이드</title><link href="http://localhost:4000/computer-science/boj11404.html" rel="alternate" type="text/html" title="[BOJ 11404] 플로이드" /><published>2025-07-24T00:00:00+09:00</published><updated>2025-07-24T00:00:00+09:00</updated><id>http://localhost:4000/computer-science/boj11404</id><content type="html" xml:base="http://localhost:4000/computer-science/boj11404.html">[https://www.acmicpc.net/problem/11404](https://www.acmicpc.net/problem/11404)

&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="computer-science" /><category term="problem-solving" /><summary type="html">https://www.acmicpc.net/problem/11404</summary></entry><entry><title type="html">Floyd–Warshall Algorithm</title><link href="http://localhost:4000/computer-science/floyd-warshall-algorithm.html" rel="alternate" type="text/html" title="Floyd–Warshall Algorithm" /><published>2025-07-24T00:00:00+09:00</published><updated>2025-07-24T00:00:00+09:00</updated><id>http://localhost:4000/computer-science/floyd-warshall-algorithm</id><content type="html" xml:base="http://localhost:4000/computer-science/floyd-warshall-algorithm.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Introduction

The **Floyd--Warshall algorithm** is a dynamic programming algorithm used to find the shortest paths between all pairs of vertices in a weighted graph.
It can handle graphs with positive and negative-edge weights but does not work with graphs containing negative cycles.
The algorithm is particularly useful for dense graphs and is often used in network routing protocols.

## Explanation

The Floyd--Warshall algorithm works by iteratively improving the shortest path estimates between all pairs of vertices.

1. Create a distance matrix `dist` where `dist[i][j]` is the weight of the edge from vertex `i` to vertex `j`. If there is no edge, set it to infinity. Set `dist[i][i] = 0` for all vertices `i`.
2. For each vertex `k`, iterate through all pairs of vertices `(i, j)` and update the distance matrix:
\\[
\text{dist}[i][j] = \min(\text{dist}[i][j], \text{dist}[i][k] + \text{dist}[k][j])
\\]

Consider the graph below:
```mermaid
graph LR
    A((A));B((B));C((C));D((D));
    A--&gt;|5|B;
    B--&gt;|4|A;
    B--&gt;|-3|C;
    C--&gt;|6|A;
    A---&gt;|7|D;
    C--&gt;|4|D;
    D--&gt;|2|C;
```

The initial distance matrix is:

| i\j   | A | B | C | D |
|-------|---|---|---|---|
| **A** | 0 | 5 | ∞ | 7 |
| **B** | 4 | 0 | -3| ∞ |
| **C** | 6 | ∞ | 0 | 4 |
| **D** | ∞ | ∞ | 2 | 0 |
{:.centered}

Afer the first iteration with `k = A`, the distance matrix becomes:

| i\j   | A | B | C | D |
|-------|---|---|---|---|
| **A** | 0 | 5 | ∞ | 7 |
| **B** | 4 | 0 | -3| 11 |
| **C** | 6 | 11| 0 | 4 |
| **D** | ∞ | ∞ | 2 | 0 |
{:.centered}

After the second iteration with `k = B`, the distance matrix becomes:

| i\j   | A | B | C | D |
|-------|---|---|---|---|
| **A** | 0 | 5 | 2 | 7 |
| **B** | 4 | 0 | -3| 11 |
| **C** | 6 | 11| 0 | 4 |
| **D** | ∞ | ∞ | 2 | 0 |
{:.centered}

After the third iteration with `k = C`, the distance matrix becomes:

| i\j   | A | B | C | D |
|-------|---|---|---|---|
| **A** | 0 | 5 | 2 | 6 |
| **B** | 3 | 0 | -3| 1 |
| **C** | 6 | 11| 0 | 4 |
| **D** | 8 | 13| 2 | 0 |
{:.centered}

After the fourth iteration with `k = D`, the final distance matrix becomes:

| i\j   | A | B | C | D |
|-------|---|---|---|---|
| **A** | 0 | 5 | 2 | 6 |
| **B** | 3 | 0 | -3| 1 |
| **C** | 6 | 11| 0 | 4 |
| **D** | 8 | 13| 2 | 0 |
{:.centered}

## Complexity

The time complexity of the Floyd--Warshall algorithm is $O(V^3)$, where $V$ is the number of vertices in the graph.
This is because the algorithm consists of three nested loops, each iterating over all vertices.
The space complexity is $O(V^2)$ due to the distance matrix.

## Code

Let&apos;s see the sample code.

```cpp
const int MAX;
const int INF;
int dist[MAX][MAX];
int N; // Number of vertices

void init(){
    for(int i=1; i&lt;=N; i++) for(int j=1; j&lt;=N; j++)
        dist[i][j] = (i==j)?0:INF;
}

voiid FloydWarshall(){
    for(int k=1; k&lt;=N; k++)
        for(int i=1; i&lt;=N; i++) for(int j=1; j&lt;=N; j++)
            dist[i][j] = min(dist[i][j],dist[i][k]+dist[k][j]);
}
```

## Applications

The Floyd--Warshall algorithm is widely used in various applications, including:
* Optimizing network routing protocols.
* Finding the transitive closure of a directed graph.
* Inverting real-valued matrices. (Gauss--Jordan elimination)</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="computer-science" /><category term="algorithms" /><summary type="html"></summary></entry><entry><title type="html">Bellman–Ford Algorithm</title><link href="http://localhost:4000/computer-science/bellman-ford-algorithm.html" rel="alternate" type="text/html" title="Bellman–Ford Algorithm" /><published>2025-07-24T00:00:00+09:00</published><updated>2025-07-24T00:00:00+09:00</updated><id>http://localhost:4000/computer-science/bellman-ford-algorithm</id><content type="html" xml:base="http://localhost:4000/computer-science/bellman-ford-algorithm.html">&lt;!--more--&gt;
* this unordered seed list will be replaced by the toc
{:toc}

## Introduction

The **Bellman--Ford algorithm** is a dynamic programming algorithm used to find the shortest paths from a single source vertex to all other vertices in a weighted graph.
It can handle graphs with negative edge weights, making it more versatile than Dijkstra&apos;s algorithm, which cannot handle negative weights.
Although it is slower than Dijkstra&apos;s algorithm, it can also detect negative weight cycles, which Dijkstra&apos;s algorithm cannot do.

## Explanation

The Bellman--Ford algorithm works by iteratively relaxing the edges of the graph.
The relaxation process updates the shortest path estimate for each vertex based on the current known shortest paths.

1. Initialize the distance to the source vertex to 0 and all other vertices to infinity.
2. Iterate through all edges and update the distance to the destination vertex if a shorter path is found.
3. Repeat the relaxation process for $V - 1$ iterations, where $V$ is the number of vertices in the graph.
4. After $V - 1$ iterations, check for negative weight cycles by iterating through all edges again. If any distance can still be updated, a negative weight cycle exists. If no updates are possible, the algorithm terminates successfully.

Consider the graph and the edges below:

```mermaid
graph LR
    A((&quot;$$A:0$$&quot;));B((&quot;$$B:\infty$$&quot;));C((&quot;$$C:\infty$$&quot;));D((&quot;$$D:\infty$$&quot;));E((&quot;$$E:\infty$$&quot;));
    A--&gt;|8|E;
    A--&gt;|-6|B;
    A--&gt;|9|D;
    B--&gt;|-2|C;
    C--&gt;|5|D;
    D--&gt;|-4|C;
    A--&gt;|3|C;
    C--&gt;|-7|E;
    E--&gt;|-13|C;
```
&lt;center&gt;
$\mathrm{E} = \{(A, E, 8), (A, B, -6), (A, D, 9), (B, C, -2), (C, D, 5), (D, C, -4), (A, C, 3), (C, E, -7), (E, C, -13)\}$
&lt;/center&gt;&lt;br&gt;

After the first relaxation, the distances are updated as follows:

```mermaid
graph LR
    A((&quot;$$A:0$$&quot;));B((&quot;$$B:-6$$&quot;));C((&quot;$$C:-28$$&quot;));D((&quot;$$D:-3$$&quot;));E((&quot;$$E:-15$$&quot;));
    A--&gt;|8|E;
    A--&gt;|-6|B;
    A--&gt;|9|D;
    B--&gt;|-2|C;
    C--&gt;|5|D;
    D--&gt;|-4|C;
    A--&gt;|3|C;
    C--&gt;|-7|E;
    E--&gt;|-13|C;
```

After the second relaxation, the distances are updated as follows:

```mermaid
graph LR
    A((&quot;$$A:0$$&quot;));B((&quot;$$B:-6$$&quot;));C((&quot;$$C:-48$$&quot;));D((&quot;$$D:-23$$&quot;));E((&quot;$$E:-35$$&quot;));
    A--&gt;|8|E;
    A--&gt;|-6|B;
    A--&gt;|9|D;
    B--&gt;|-2|C;
    C--&gt;|5|D;
    D--&gt;|-4|C;
    A--&gt;|3|C;
    C--&gt;|-7|E;
    E--&gt;|-13|C;
```

After the third relaxation, the distances are updated as follows:

```mermaid
graph LR
    A((&quot;$$A:0$$&quot;));B((&quot;$$B:-6$$&quot;));C((&quot;$$C:-68$$&quot;));D((&quot;$$D:-43$$&quot;));E((&quot;$$E:-55$$&quot;));
    A--&gt;|8|E;
    A--&gt;|-6|B;
    A--&gt;|9|D;
    B--&gt;|-2|C;
    C--&gt;|5|D;
    D--&gt;|-4|C;
    A--&gt;|3|C;
    C--&gt;|-7|E;
    E--&gt;|-13|C;
```

Likewise, for this graph, the Bellman--Ford algorithm will continue to relax the edges infinitely.
Therefore, it will detect a negative weight cycle at the end of the algorithm.

## Complexity

The time complexity of the Bellman--Ford algorithm is $O(VE)$, where $V$ is the number of vertices and $E$ is the number of edges in the graph.
The space complexity is $O(V)$, as it requires storage for the distance estimates and predecessor information for each vertex.

## Code

Let&apos;s see the sample code.

```cpp
const int MAX;
const int INF;
struct edge{
    int u,v,w;
};
vector&lt;edge&gt; G;
int dis[MAX];
int V,E;

void init(){
    for(int i=1; i&lt;=V; i++) dis[i] = INF;
}

bool BellmanFord(int K){
    dis[K] = 0;
    for(int i=1; i&lt;=V; i++) for(auto [u,v,w]:G)
        if(dis[u]!=INF and dis[u]+w&lt;dis[v]){
            dis[v] = dis[u]+w;
            if(i==V) return false;
        }
    return true;
}
```

## Applications

The Bellman--Ford algorithm is widely used in various applications, including:
- **Network routing protocols**: It is used in protocols like RIP (Routing Information Protocol) to find the shortest paths in networks.
- **Graph analysis**: It is used to analyze graphs with negative edge weights and detect negative weight cycles.</content><author><name>Jiho Jun</name><email>pianoforte0203@gmail.com</email></author><category term="computer-science" /><category term="algorithms" /><summary type="html"></summary></entry></feed>